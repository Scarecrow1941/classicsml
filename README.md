# classicsml ⛈️🔄📉
classicsml : Classsic Supervised ML # Cost Function # Gradient Descent


## Objective
- Implement supervised learning
- Minimized error with cost function
  - SSD (sum of squared differences)
  - SAD (sum of absolute differences)
- Optimized model using gradient descent
- Experiment with learning rates (how they affect training)

## Classical ML models with Supervised Learning

![classicsml001.png](./media/classicsml001.png)

![classicsml002.png](./media/classicsml002.png)

![classicsml003.png](./media/classicsml003.png)

![classicsml004.png](./media/classicsml004.png)

![classicsml005.png](./media/classicsml005.png)

![classicsml006.png](./media/classicsml006.png)

